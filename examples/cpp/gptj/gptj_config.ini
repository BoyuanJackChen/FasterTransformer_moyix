[ft_instance_hyperparameter]
max_batch_size=1 ; Use for allocate the buffer
max_seq_len=2048 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=0 ; k value for top k sampling
top_p=1.0 ; p value for top p sampling
temperature=0.2 ; Use for sampling
repetition_penalty=1.0 ; Use for sampling
len_penalty=1.0
beam_search_diversity_rate=0.0
is_half=1
enable_custom_all_reduce=0

tensor_para_size=1
pipeline_para_size=1

;model_name=codegen_6B
;model_dir=/fastdata/gpt-j-6b/codegen-350M-multi
;model_dir=/fastdata/gpt-j-6b/codegen-350M-nl
;model_dir=/fastdata/gpt-j-6b/codegen-350M-mono
;model_dir=/fastdata/gpt-j-6b/codegen-2B-multi
;model_dir=/fastdata/gpt-j-6b/codegen-2B-nl
;model_dir=/fastdata/gpt-j-6b/codegen-2B-mono
;model_dir=/fastdata/gpt-j-6b/codegen-16B-multi
;model_dir=/fastdata/gpt-j-6b/codegen-6B-multi
;model_dir=/fastdata/gpt-j-6b/codegen-6B-nl
;model_dir=/fastdata/gpt-j-6b/codegen-6B-mono
;model_dir=/fastdata/gpt-j-6b/codegen-16B-multi
;model_dir=/fastdata/gpt-j-6b/codegen-16B-nl
;model_dir=/fastdata/gpt-j-6b/codegen-16B-mono
model_name=gptj_6B
model_dir=/fastdata/gpt-j-6b/gptj-6b-converted

[request]
request_batch_size=1 # determine by the request
request_output_len=2040 # determine by the request

[gptj_6B]
head_num=16
size_per_head=256
vocab_size=50400
decoder_layers=28
rotary_embedding=64
start_id=50256
end_id=50256
inter_size=16384

[codegen_6B]
head_num=16
size_per_head=256
vocab_size=51200
decoder_layers=33
rotary_embedding=64
start_id=50256
end_id=50256
inter_size=16384

[codegen_2B]
head_num=32
size_per_head=80
vocab_size=51200
decoder_layers=32
rotary_embedding=64
start_id=50256
end_id=50256
inter_size=10240

[codegen_350M]
head_num=16
size_per_head=64
vocab_size=51200
decoder_layers=20
rotary_embedding=32
start_id=50256
end_id=50256
inter_size=4096

[codegen_16B]
head_num=24
size_per_head=256
vocab_size=51200
decoder_layers=34
rotary_embedding=64
start_id=50256
end_id=50256
inter_size=24576
